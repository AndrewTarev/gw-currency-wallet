package kafka

import (
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/segmentio/kafka-go"
)

const (
	maxRetries    = 5                  // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–µ—Ç—Ä–∞–µ–≤
	retryInterval = 2 * time.Second    // –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
	dlqTopic      = "transactions-dlq" // Dead Letter Queue (DLQ)
)

type KafkaConsumer struct {
	reader   *kafka.Reader
	producer *kafka.Writer
	metrics  *ConsumerMetrics
}

// ConsumerMetrics - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è Prometheus –º–µ—Ç—Ä–∏–∫
type ConsumerMetrics struct {
	processedMessages prometheus.Counter
	failedMessages    prometheus.Counter
}

func NewConsumerMetrics() *ConsumerMetrics {
	m := &ConsumerMetrics{
		processedMessages: prometheus.NewCounter(prometheus.CounterOpts{
			Name: "consumer_processed_messages",
			Help: "Total number of processed messages",
		}),
		failedMessages: prometheus.NewCounter(prometheus.CounterOpts{
			Name: "consumer_failed_messages",
			Help: "Total number of failed messages",
		}),
	}
	prometheus.MustRegister(m.processedMessages, m.failedMessages)
	return m
}

// NewKafkaConsumer —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π Kafka Consumer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π DLQ –∏ –º–µ—Ç—Ä–∏–∫
func NewKafkaConsumer(brokers []string, topic, groupID string) *KafkaConsumer {
	return &KafkaConsumer{
		reader: kafka.NewReader(kafka.ReaderConfig{
			Brokers:     brokers,
			Topic:       topic,
			GroupID:     groupID,
			MinBytes:    10e3, // 10KB
			MaxBytes:    10e6, // 10MB
			MaxWait:     500 * time.Millisecond,
			StartOffset: kafka.FirstOffset,
		}),
		producer: &kafka.Writer{
			Addr:         kafka.TCP(brokers...),
			Topic:        dlqTopic,
			BatchSize:    1, // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏
			BatchTimeout: 10 * time.Millisecond,
		},
		metrics: NewConsumerMetrics(),
	}
}

// processMessage –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
func (kc *KafkaConsumer) processMessage(msg kafka.Message) error {
	// ‚ùå –°–∏–º—É–ª—è—Ü–∏—è –æ—à–∏–±–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
	if string(msg.Value) == "error" {
		return fmt.Errorf("simulated error")
	}

	// ‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ
	log.Printf("‚úÖ Processed message: %s", string(msg.Value))
	kc.metrics.processedMessages.Inc()
	return nil
}

// retryProcessing –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —Ä–µ—Ç—Ä–∞—è–º–∏
func (kc *KafkaConsumer) retryProcessing(ctx context.Context, msg kafka.Message) {
	for i := 0; i < maxRetries; i++ {
		err := kc.processMessage(msg)
		if err == nil {
			return // ‚úÖ –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º
		}
		log.Printf("üîÑ Retry %d/%d: %v\n", i+1, maxRetries, err)
		time.Sleep(retryInterval)
	}

	// ‚ùå –ü–æ—Å–ª–µ maxRetries –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ DLQ
	err := kc.producer.WriteMessages(ctx, kafka.Message{
		Key:   msg.Key,
		Value: msg.Value,
	})
	if err != nil {
		log.Printf("‚ùå Failed to send to DLQ: %v\n", err)
	} else {
		log.Printf("‚ò†Ô∏è Message sent to DLQ: %s\n", string(msg.Value))
	}
	kc.metrics.failedMessages.Inc()
}

// Start –∑–∞–ø—É—Å–∫–∞–µ—Ç consumer
func (kc *KafkaConsumer) Start() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	go func() {
		for {
			msg, err := kc.reader.ReadMessage(ctx)
			if err != nil {
				log.Printf("‚ùå Error reading message: %v\n", err)
				continue
			}

			// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å —Ä–µ—Ç—Ä–∞—è–º–∏
			kc.retryProcessing(ctx, msg)
		}
	}()

	// –û–±—Ä–∞–±–æ—Ç–∫–∞ SIGINT/SIGTERM
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	<-sigChan
	fmt.Println("üõë Shutting down consumer...")
	kc.reader.Close()
	kc.producer.Close()
}
